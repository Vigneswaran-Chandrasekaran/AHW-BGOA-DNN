{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Haar Wavelet based Binary Grasshopper Optimization algorithm for Finetuning Deep Neural Network hyper praramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback,EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Dataset for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1= genfromtxt('Focal_Db10_features.csv', delimiter=',', skip_header=0)\n",
    "f2= genfromtxt('NFocal_Db10_features.csv', delimiter=',', skip_header=0)\n",
    "#make them as list and add them to make dataset X\n",
    "focal=list(f1)\n",
    "non_focal=list(f2)\n",
    "X=numpy.array(focal+non_focal)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "#add class label for them\n",
    "y_focal=[0]*len(focal)\n",
    "y_nfocal=[1]*len(non_focal)\n",
    "Y=numpy.array(y_focal+y_nfocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Definition for Optimizer Changer, used as a callback to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerChanger(Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if epoch==200:            \n",
    "            print(\"Triggered Changing to learning rate annelaing with SGD\")\n",
    "            sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\n",
    "            model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "            epoch_size=200;batch_size=2048        \n",
    "            #callback initialization for learning rate annealing \n",
    "            schedule =SGDRScheduler(min_lr=min_lr_value,max_lr=max_lr_value,steps_per_epoch=numpy.ceil(epoch_size/batch_size),lr_decay=0.9,cycle_length=10,mult_factor=1.5)    \n",
    "            history=model.fit(X_train, Y_train,validation_data=(X_valid,Y_valid),epochs=200,batch_size=2048,callbacks=[schedule],verbose=0,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Definition for SGD + Learning rate Annealing, used as a callback to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "\n",
    "    def __init__(self,min_lr,max_lr,steps_per_epoch,lr_decay=1,cycle_length=10,mult_factor=2):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + numpy.cos(fraction_to_restart * numpy.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = numpy.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        print(\"Evaluating the model for Generalization Error over unseen data\")\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "        cv.append(scores[1])\n",
    "        print(scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neccesary variables and initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold validation data split\n",
    "skfold = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "cv=[]\n",
    "#value for hyperparameters for Learning rate Annealing\n",
    "max_lr_value=1e-1\n",
    "min_lr_value=1e-4\n",
    "momentum_value=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiliation of model [Deep NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggered Changing to learning rate annelaing with SGD\n",
      "Evaluating the model for Generalization Error over unseen data\n",
      "750/750 [==============================] - 0s 161us/step\n",
      "0.9106666668256124\n",
      "Triggered Changing to learning rate annelaing with SGD\n",
      "Evaluating the model for Generalization Error over unseen data\n",
      "750/750 [==============================] - 0s 63us/step\n",
      "0.9253333333333333\n",
      "Triggered Changing to learning rate annelaing with SGD\n",
      "Evaluating the model for Generalization Error over unseen data\n",
      "750/750 [==============================] - 0s 68us/step\n",
      "0.9306666661898295\n",
      "Triggered Changing to learning rate annelaing with SGD\n",
      "Evaluating the model for Generalization Error over unseen data\n",
      "750/750 [==============================] - 0s 83us/step\n",
      "0.9240000001589457\n",
      "Triggered Changing to learning rate annelaing with SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vigneswaranradheyan/.local/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.119006). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    }
   ],
   "source": [
    "for (train,test) in skfold.split(X,Y):        \n",
    "    #data split for training and validation \n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X[train],Y[train], test_size=0.2, shuffle= True,stratify=Y[train])\n",
    "    #define model\n",
    "    model = Sequential()\n",
    "    #input layer \n",
    "    model.add(Dense(100, input_dim=X.shape[1], activation='relu',kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    #Hidden layer 1\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 2\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #hidden layer 3\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 4\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 5\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 6\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile the model\n",
    "    stopper=OptimizerChanger()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "    history=model.fit(X_train, Y_train,validation_data=(X_valid,Y_valid),epochs=201,batch_size=2048,callbacks=[stopper],verbose=0,shuffle=True)\n",
    "    #define optimizing algorithm setting\n",
    "    #model fitting for the above settings\n",
    "    #Evaluate the trained model on unseen test dataset for checking generalization\n",
    "    \n",
    "print(cv)\n",
    "sss=sum(cv)/float(len(cv))\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
