{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Haar wavelet based Binary Grasshopper Optimization Algorithm for  optimizing hyper parameters of Deep Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters to be optimized are:\n",
    "\n",
    "1) Dropout regularization rate\n",
    "2) L1 regularization rate\n",
    "Cosine Learning rate Annealing:\n",
    "3) Maximum learning rate \n",
    "4) Minimum learning rate\n",
    "5) Exponential decay rate\n",
    "Haar wavelet family selection hyperparameter:\n",
    "6) Haar wavelet k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping,TensorBoard,Callback\n",
    "from keras import regularizers\n",
    "from random import randint\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import numpy,math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class OptimizerChange to change model to compile from Adamax to SGD + Cosine Learning rate Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerChanger(Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        epoch_size=second_epochcount\n",
    "        batch_size=2048            \n",
    "        if epoch==first_epochcount:            \n",
    "            print(\"Changing to learning rate annelaing with SGD\")\n",
    "            sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\n",
    "            model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "            #callback initialization for learning rate annealing \n",
    "            schedule =SGDRScheduler(min_lr=min_lr_value,max_lr=max_lr_value,steps_per_epoch=numpy.ceil(epoch_size/batch_size),lr_decay=0.70,cycle_length=10,mult_factor=1.5)    \n",
    "            history=model.fit(X_train, Y_train,validation_data=(X_valid,Y_valid),epochs=second_epochcount,batch_size=2048,callbacks=[schedule],verbose=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class SGD Scheduler for LR Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    def __init__(self,min_lr,max_lr,steps_per_epoch,lr_decay=1,cycle_length=10,mult_factor=2):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "        self.history = {}\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + numpy.cos(fraction_to_restart * numpy.pi))\n",
    "        return lr\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = numpy.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "        cv.append(scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functio for Commanility Based Crossover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonality_based_crossover(x):\n",
    "    y=[]\n",
    "    x=numpy.array(x)\n",
    "    l=x.shape[1]\n",
    "    flag=numpy.all(x==x[0,:],axis=0)\n",
    "    x=x.T\n",
    "    for i in range(l):\n",
    "        if flag[i]!=True:\n",
    "            y.append(i)\n",
    "    z=(x[y,:].T)\n",
    "    z=numpy.reshape(z,[1,z.shape[0]*z.shape[1]])\n",
    "    z=numpy.reshape(z,x[y,:].shape)\n",
    "    xx=[0*n for n in range(l)]\n",
    "    count=0\n",
    "    for i in range(l):\n",
    "        if flag[i]!=True:\n",
    "            xx[i]=list(z[count])\n",
    "            count+=1\n",
    "        else:\n",
    "            xx[i]=list(x[i])\n",
    "    xx=numpy.array(xx).T\n",
    "    return(list(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Haar wavelet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_haar_wavelet(binary,k_value):\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    binary = min_max_scaler.fit_transform(binary)\n",
    "    binary=binary.tolist()\n",
    "    for i in range(len(binary)):\n",
    "        m=k_value[i]\n",
    "        m=(2**m)\n",
    "        if m<=1:\n",
    "            m=2\n",
    "        k=randint(0,m//2)\n",
    "        for j in range(len(binary[i])):\n",
    "            x=binary[i][j]\n",
    "            if x >= (k/float(m)) and x <= ((k+0.5)/float(m)):\n",
    "                binary[i][j]=int(1)\n",
    "            else:\n",
    "                binary[i][j]=int(0)\n",
    "    return(binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot loss and accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history):\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('loss_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for changing parameters to appropraite range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_calc(bits):\n",
    "    Dropout_rate=bits[0:5]\n",
    "    l1reg_rate=bits[6:10]\n",
    "    max_lr_value=bits[10:13]\n",
    "    min_lr_value=bits[13:16]\n",
    "    decay_rate=bits[16:19]\n",
    "    #Dropout rate\n",
    "    Dropout_rate=[str(int(i)) for i in Dropout_rate]\n",
    "    Dropout_rate=''.join(Dropout_rate)\n",
    "    Dropout_rate=int(Dropout_rate,2)\n",
    "    if Dropout_rate==0:\n",
    "        Dropout_rate=5\n",
    "    Dropout_rate=Dropout_rate/float(100)\n",
    "    #L1 regularization rate \n",
    "    l1reg_rate=[str(int(i)) for i in l1reg_rate]\n",
    "    l1reg_rate=''.join(l1reg_rate)\n",
    "    l1reg_rate=int(l1reg_rate,2)\n",
    "    if l1reg_rate==0:\n",
    "        l1reg_rate=5\n",
    "    l1reg_rate=l1reg_rate/float(1000)\n",
    "    #Max Lr\n",
    "    max_lr_value=[str(int(i)) for i in max_lr_value]\n",
    "    max_lr_value=''.join(max_lr_value)\n",
    "    max_lr_value=int(max_lr_value,2)\n",
    "    if max_lr_value==0:\n",
    "        max_lr_value=1\n",
    "    max_lr_value=max_lr_value*(10**-2)\n",
    "    #Min lr\n",
    "    min_lr_value=[str(int(i)) for i in min_lr_value]\n",
    "    min_lr_value=''.join(min_lr_value)\n",
    "    min_lr_value=int(min_lr_value,2)\n",
    "    if min_lr_value==0:\n",
    "        min_lr_value=1\n",
    "    min_lr_value=min_lr_value*(10**-5)\n",
    "    #Exponential Decay rate\n",
    "    decay_rate=[str(int(i)) for i in decay_rate]\n",
    "    decay_rate=''.join(decay_rate)\n",
    "    decay_rate=int(decay_rate,2)\n",
    "    if min_lr_value==0:\n",
    "        min_lr_value=1\n",
    "    decay_rate=decay_rate(10**-2)\n",
    "    return(Dropout_rate,l1reg_rate,max_lr_value,min_lr_value,decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNeuralNetwork(list_with_one,bits,X,Y): \n",
    "    skfold = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "    Dropout_rate,l1reg_rate,max_lr_value,min_lr_value,decay_rate = param_calc(bits)\n",
    "    cvscore=[]    \n",
    "    for (train, test) in skfold.split(X,Y):\n",
    "        #data split for training and validation \n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X[train],Y[train], test_size=0.2, shuffle= True,stratify=Y[train])\n",
    "    #define model\n",
    "    model = Sequential()\n",
    "    #input layer \n",
    "    model.add(Dense(100, input_dim=X.shape[1], activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
    "    #Hidden layer 1\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 2\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #hidden layer 3\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 4\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 5\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    #Hidden layer 6\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile the model\n",
    "    stopper=OptimizerChanger()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "    history=model.fit(X_train, Y_train,validation_data=(X_valid,Y_valid),epochs=101,batch_size=2048,callbacks=[stopper],verbose=0,shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grasshopper Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goa_haar_dffnn(no_of_generation,no_of_population,bitgenerated,feature_size,X,Y):\n",
    "    binary_bits=[[]*n for n in range(no_of_population)]\n",
    "    cMax=1\n",
    "    cMin=0.00001\n",
    "    best_fitness=-100.00\n",
    "    best_accuracy=-100.00\n",
    "    fitness_array=[]\n",
    "    best_fitness_monitor=[]\n",
    "    for j in range(no_of_population):\n",
    "        for k in range(bitgenerated):\n",
    "            x=randint(0,1)\n",
    "            binary_bits[j].append(x)\n",
    "    print(\"Generation0:\")\n",
    "    for index in range(no_of_population):\n",
    "        list_with_one=[]\n",
    "        for t in range(feature_size):\n",
    "            if binary_bits[index][t]==1:\n",
    "                list_with_one.append(t)\n",
    "        if len(list_with_one)<=5:\n",
    "            list_with_one=(range(20,35))\n",
    "        accuracy=DeepNeuralNetwork(list_with_one,binary_bits[index][feature_size:-4],X[:,list_with_one],Y)\n",
    "        print(accuracy)\n",
    "        fitnessA=(0.9099*accuracy)\n",
    "        fitnessB=0.0001*(1-(len(list_with_one)/float(feature_size)))\n",
    "        fitness=fitnessA+fitnessB\n",
    "        fitness_array.append(fitness)\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness=fitness\n",
    "            best_fitness_index=index\n",
    "            best_accuracy=accuracy\n",
    "    print(\"Best fitness: \"+str(best_fitness)+\" and \"+\"Best accuracy: \"+str(best_accuracy))\n",
    "    best_fitness_monitor.append(best_fitness)\n",
    "        for index in range(no_of_generation-1):\n",
    "        print(\"Generation\"+str(index+1)+':')\n",
    "        c=cMax-(index+1)*((cMax-cMin)/float(no_of_generation))\n",
    "        k_value=[]\n",
    "        for pop in range(no_of_population):    \n",
    "            k_instant=[str(ith) for ith in binary_bits[pop][-4:-1]]\n",
    "            k_instant=''.join(k_instant)\n",
    "            k_value.append(int(k_instant,2))\n",
    "            for dec in range(bitgenerated):\n",
    "                summation=0\n",
    "                current_bit=binary_bits[pop][dec]\n",
    "                for k in range(no_of_population):\n",
    "                    if k!=pop:\n",
    "                        a=abs(current_bit-binary_bits[k][dec])\n",
    "                        b=current_bit-binary_bits[k][dec]\n",
    "                        a=round(a,3)\n",
    "                        summation+=0.5*c*math.exp(a/1.5)*0.5-math.exp(-a)*b*3*current_bit\n",
    "                binary_bits[pop][dec]=c*summation+binary_bits[best_fitness_index][dec]\n",
    "        binary_bits=adaptive_haar_wavelet(binary_bits,k_value)\n",
    "        for t in range(no_of_population):\n",
    "            list_with_one=[]\n",
    "            for s in range(feature_size):\n",
    "                if binary_bits[t][s]==1:\n",
    "                    list_with_one.append(s)\n",
    "            if len(list_with_one)<=5:\n",
    "                list_with_one=(range(20,35))\n",
    "            accuracy=DeepNeuralNetwork(list_with_one,binary_bits[t][feature_size:-4],X[:,list_with_one],Y)\n",
    "            print(accuracy)\n",
    "            fitnessA=(0.9099*accuracy)\n",
    "            fitnessB=0.0001*(1-(len(list_with_one)/float(feature_size)))\n",
    "            fitness=fitnessA+fitnessB\n",
    "            fitness_array.append(fitness)\n",
    "            if fitness > best_fitness:\n",
    "                best_fitness=fitness\n",
    "                best_fitness_index=index\n",
    "                best_accuracy=accuracy\n",
    "        print(\"Best fitness: \"+str(best_fitness)+\" and \"+\"Best accuracy: \"+str(best_accuracy))\n",
    "        best_fitness_monitor.append(best_fitness)\n",
    "        if len(best_fitness_monitor)>=4 and best_fitness_monitor[-1]==best_fitness_monitor[-2]==best_fitness_monitor[-3]:\n",
    "            print(\"Maximum fitness value is repeated for three times, Initiating Crossover\")\n",
    "            binary_bits=commonality_based_crossover(binary_bits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the execution with necessary variables and importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_generation=10\n",
    "no_of_population=3\n",
    "max_lr_len=3\n",
    "min_lr_len=3\n",
    "decay_rate\n",
    "l1reg_rate=4\n",
    "drop_out=6\n",
    "haar_para_len=4\n",
    "feature_size=37\n",
    "bitgenerated=max_lr_len+min_lr_len+decay_rate+drop_out+haar_para_len+feature_size+l1reg_rate\n",
    "#Import data from the .CSV file\n",
    "f1= genfromtxt('Focal_Db10_features.csv', delimiter=',', skip_header=0)\n",
    "f2= genfromtxt('NFocal_Db10_features.csv', delimiter=',', skip_header=0)\n",
    "#make them as list and add them to make dataset X\n",
    "focal=list(f1)\n",
    "non_focal=list(f2)\n",
    "X=numpy.array(focal+non_focal)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "print(X.shape)\n",
    "#add class label for them\n",
    "y_focal=[0]*len(focal)\n",
    "y_nfocal=[1]*len(non_focal)\n",
    "Y=numpy.array(y_focal+y_nfocal)\n",
    "goa_haar_dffnn(no_of_generation,no_of_population,bitgenerated,feature_size,X,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
